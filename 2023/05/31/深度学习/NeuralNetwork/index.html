<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Neural Network（已废弃，见最新文章） | Linermao's kiosk</title><meta name="author" content="Linermao,86281366@qq.com"><meta name="copyright" content="Linermao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="神经网络Neural Network 前言:最近学习了神经网络的算法, 想要整合一下脑子中所有的想法, 顺便梳理一下思路, 于是写下这篇文章.本篇文章代码基于c语言, 从最简单的二分类问题入手. 什么是神经网络:神经网络(Neural Network,NN)一般指人工神经网络(Artificial Neural Network,ANN). 神经网络的名称和结构均受到人脑的启发, 通过模仿生物神经元">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Network（已废弃，见最新文章）">
<meta property="og:url" content="http://example.com/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/index.html">
<meta property="og:site_name" content="Linermao&#39;s kiosk">
<meta property="og:description" content="神经网络Neural Network 前言:最近学习了神经网络的算法, 想要整合一下脑子中所有的想法, 顺便梳理一下思路, 于是写下这篇文章.本篇文章代码基于c语言, 从最简单的二分类问题入手. 什么是神经网络:神经网络(Neural Network,NN)一般指人工神经网络(Artificial Neural Network,ANN). 神经网络的名称和结构均受到人脑的启发, 通过模仿生物神经元">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/Avatar.jpg">
<meta property="article:published_time" content="2023-05-30T16:00:00.000Z">
<meta property="article:modified_time" content="2023-07-25T12:13:35.586Z">
<meta property="article:author" content="Linermao">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/Avatar.jpg"><link rel="shortcut icon" href="/images/favicon.jpg"><link rel="canonical" href="http://example.com/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d0547acfa82ccac854b804414ae584ec";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Neural Network（已废弃，见最新文章）',
  isPost: true,
  isHome: false,
  isHighlightShrink: undefined,
  isToc: true,
  postUpdate: '2023-07-25 20:13:35'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/Avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/index.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Linermao's kiosk"><span class="site-name">Linermao's kiosk</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Neural Network（已废弃，见最新文章）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-30T16:00:00.000Z" title="发表于 2023-05-31 00:00:00">2023-05-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-07-25T12:13:35.586Z" title="更新于 2023-07-25 20:13:35">2023-07-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Neural Network（已废弃，见最新文章）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="神经网络Neural-Network"><a href="#神经网络Neural-Network" class="headerlink" title="神经网络Neural Network"></a>神经网络Neural Network</h1><hr>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言:"></a>前言:</h2><p>最近学习了神经网络的算法, 想要整合一下脑子中所有的想法, 顺便梳理一下思路, 于是写下这篇文章.<br>本篇文章代码基于c语言, 从最简单的二分类问题入手.</p>
<h3 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络:"></a>什么是神经网络:</h3><p>神经网络(Neural Network,NN)一般指人工神经网络(Artificial Neural Network,ANN).</p>
<p>神经网络的名称和结构均受到人脑的启发, 通过模仿生物神经元相互传递信号的方式进行机器学习, 从而让机器掌握与神经结构类似的反应机制.</p>
<div align="center">
<img width="850" height="120" alt="对比表" src="/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/对比表.png">
<p style="font-size:14px;text-align: center;">图1-1</p></div>


<div align="center">
<img width="300" height="150" alt="生物神经元" src="/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/生物神经元.jpg">
<p style="font-size:14px;">图1-2</p></div>


<div align="center">
<img width="300" height="150" alt="人工神经网络" src="/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/人工神经网络.jpg">
<p style="font-size:14px;">图1-3</p></div>


<p>如同生物神经元有许多输入(树突)一样, 人工神经元也有很多输入信号, 并同时作用到人工神经元上, 生物神经元中大量的突触具有不同的性质和强度, 使得不同的输入的激励作用各不相同, 因此在人工神经元中, 对每一个输入都有一个可变的加权$\omega$, 用于模拟生物神经元中突触的不同连接强度及突触的可变传递特性. 因此我们构建一个由节点层组成的人工神经网络, 包含一个输入层, 一个或多个隐藏层和一个输出层. 每一层称为一个人工神经元, 它们连接到另一个层,具有相关的权重和阈值.</p>
<div align="center">
<img alt="人工神经网络" src="/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/NN.jpg">
<p style="font-size:14px;">图1-4</p></div>


<h3 id="正向传播神经网络"><a href="#正向传播神经网络" class="headerlink" title="正向传播神经网络:"></a>正向传播神经网络:</h3><h4 id="什么是正向传播神经网络"><a href="#什么是正向传播神经网络" class="headerlink" title="什么是正向传播神经网络:"></a>什么是正向传播神经网络:</h4><p>前向传播(Forward Propagation)就是从Input, 经过一层层的Layer, 不断计算每一层的Z和A, 最后得到输出y的过程.</p>
<h4 id="正向传播是如何进行的"><a href="#正向传播是如何进行的" class="headerlink" title="正向传播是如何进行的:"></a>正向传播是如何进行的:</h4><p>在图1-4中, 我们向隐藏层输入了已知的参数xi, 使用$z = \omega x + b$($z ,\omega, b$ 均表示向量 $z = [z1,z2…,z3]$)来计算第一层的神经元的值. 然后让每一层通过<strong>激活函数</strong>, $a = g(z)$ 得到激活层的值, 再将此激活层作为下一层的输入值继续上述过程, 最后到达输出层, 给出预测值.<br>神经网络的权重在正向传播过程中, 由上一层到下一层的连接权重以及每个神经元之间的边权重共同确定, 这些权重都是在训练中通过<strong>反向传播</strong>和<strong>梯度下降法</strong>进行更新的, 在反向传播过程中再讲解.</p>
<h4 id="什么是激活函数"><a href="#什么是激活函数" class="headerlink" title="什么是激活函数:"></a>什么是激活函数:</h4><p>激活函数(Activation Function)是一种添加到人工神经网络中的函数, 旨在帮助神经网络学习数据中复杂关系. 因为神经网络中每一层的输入输出都是一个线性求和的过程, 下一层的输出只是承接了上一层输入函数的线性变换, 所以如果没有激活函数, 那么无论你构造的神经网络多么复杂, 最后的输出都是输入的线性组合, 纯粹的线性组合并不能够解决更为复杂的问题. 而引入非线性的激活函数之后, 会给神经元引入非线性元素, 使得神经网络可以逼近其他的任何非线性函数, 这样可以使得神经网络应用到更多模型中.<br>常见的激活函数: <strong>Sigmoid激活函数, Relu激活函数, Tanh(双曲正切)激活函数.</strong><br>下面主要以Sigmoid函数为例:</p>
<p>Sigmoid函数也叫Logistic函数, 用于隐层神经元输出 ,取值范围为(0,1), 它可以将一个实数映射到(0,1)的区间, 可以用来做二分类. 在特征相差比较复杂或是相差不是特别大时效果比较好.<br> 函数的表达式如下：</p>
<script type="math/tex; mode=display">
    f (z) = \frac{1}{1 + e^{-z}}</script><p>图像类似一个S形曲线:</p>
<div align="center">
<img alt="人工神经网络" src="/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/Sigmoid.png">
<p style="font-size:14px;">图1-4</p></div>


<p>不同的激活函数有不同的适用范围, 这方面具体的内容在之后的文章中(如果有时间做的话)再做说明.</p>
<h3 id="如何搭建一个正向传播神经网络-以c语言为例"><a href="#如何搭建一个正向传播神经网络-以c语言为例" class="headerlink" title="如何搭建一个正向传播神经网络(以c语言为例):"></a>如何搭建一个正向传播神经网络(以c语言为例):</h3><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置:"></a>参数设置:</h4><p>从零开始搭建一个神经网络, 我们首先要确定神经网络的层数, 每一层神经元的个数.<br>这里以两层隐藏层为例:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_HIDDEN_1 20             <span class="comment">// 隐藏层1的神经元个数</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_HIDDEN_2 10             <span class="comment">// 隐藏层2的神经元个数</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_OUTPUT 1                <span class="comment">// 输出层的个数</span></span></span><br></pre></td></tr></table></figure></p>
<h4 id="数据的初始化"><a href="#数据的初始化" class="headerlink" title="数据的初始化:"></a>数据的初始化:</h4><p>对于一个还没经过训练的神经网络, 我们需要先对权重的初始化, 使其附上一定的初值, 再进行正向传播.<br>由于我们使用的激活函数是Sigmoid函数, 在x过大或者过小的时候会丢失一定的有用信息, 所以我们希望权重的值分布在一个较小的范围内.<br>在每一步, 权重矩阵乘以来自前一层的激活值. 如果每一层的权重大于1, 并且导致激活值的大小大于1, 当它们被重复乘以多次时，它们就会不断变大，甚至到无穷大. 类似地, 如果权重小于1, 它们将消失为零. 这叫做渐变爆炸和渐变消失问题. 可以类比$ 1.1 ^ {100}$ 和 $ 0.9 ^ {100}$.</p>
<p>接下来介绍Xavier初始化方法:</p>
<p>Xavier初始化的主要思想是在初始化权重时尽可能保持每个神经元输出方差相等. 简单来说, 它会随机选择每个权重值, 并根据某种特定的分布对其进行缩放, 以使分布的方差在反向传播过程中保持不变. 实际上, Xavier初始化隐含了一个假设, 那就是前一层输出和该层输入的方差相等. 通过这种初始化方式, 可以使得每个神经元之间的输出方差尽量一致, 防止梯度消失或梯度爆炸问题的发生. 因此, 使用Xavier初始化可以在训练深度神经网络的时候更容易地实现收敛, 并提高训练速度和模型性能.</p>
<p>当使用Xavier初始化时，我们首先需要确定要初始化的层中神经元的数量num1，以及该层前一层中神经元的数量num2。然后，我们可以使用以下公式来计算权重矩阵的标准差σ：</p>
<script type="math/tex; mode=display">
    \sigma = \sqrt{\frac{2.0}{num1 + num2}}</script><p>接着, 我们将权重矩阵样本自一个以0为中心, 标准差为σ的均方分布随机抽取. 这些随机选择的权重值将成为该层权重矩阵的初始值,从而用于神经网络的训练过程. 代码中的hidden代表隐藏层, 也就是A层, 激活层.</p>
<p>代码如下:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* 权重 */</span></span><br><span class="line"><span class="type">double</span> Hidden_Weight_1[DIMENSION_SIZE * NUM_HIDDEN_1];</span><br><span class="line"><span class="type">double</span> Hidden_Weight_2[NUM_HIDDEN_1 * NUM_HIDDEN_2];</span><br><span class="line"><span class="type">double</span> Output_Weight[NUM_HIDDEN_2 * NUM_OUTPUT];</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 偏置 */</span></span><br><span class="line"><span class="type">double</span> Hidden_Bias_1[DIMENSION_SIZE * NUM_HIDDEN_1];</span><br><span class="line"><span class="type">double</span> Hidden_Bias_2[NUM_HIDDEN_1 * NUM_HIDDEN_2];</span><br><span class="line"><span class="type">double</span> Output_Bias[NUM_HIDDEN_2 * NUM_OUTPUT];</span><br><span class="line"></span><br><span class="line"><span class="comment">/* weights 是要更改的值 count 是weight的数组大小 num1 上一层神经元数量 num2 当前层神经元数量 */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">xavier_init</span><span class="params">(<span class="type">double</span>* weights, <span class="type">int</span> count, <span class="type">int</span> num1, <span class="type">int</span> num2)</span> &#123;</span><br><span class="line">    <span class="type">double</span> stddev = <span class="built_in">sqrt</span>(<span class="number">2.0</span> / (num1 + num2));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</span><br><span class="line">        weights[i] = stddev * rand() / RAND_MAX;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 初始化权重和偏置,使用xavier方法 */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Init_Weight</span><span class="params">()</span>&#123;</span><br><span class="line">    xavier_init(Hidden_Weight_1, DIMENSION_SIZE * NUM_HIDDEN_1, DIMENSION_SIZE, NUM_HIDDEN_1);</span><br><span class="line">    xavier_init(Hidden_Weight_2, NUM_HIDDEN_1 * NUM_HIDDEN_2,   NUM_HIDDEN_1,   NUM_HIDDEN_2);</span><br><span class="line">    xavier_init(Output_Weight,   NUM_HIDDEN_2 * NUM_OUTPUT,     NUM_HIDDEN_2,   NUM_OUTPUT);</span><br><span class="line"></span><br><span class="line">    xavier_init(Hidden_Bias_1, DIMENSION_SIZE * NUM_HIDDEN_1, DIMENSION_SIZE, NUM_HIDDEN_1);</span><br><span class="line">    xavier_init(Hidden_Bias_2, NUM_HIDDEN_1 * NUM_HIDDEN_2,   NUM_HIDDEN_1,   NUM_HIDDEN_2);</span><br><span class="line">    xavier_init(Output_Bias,   NUM_HIDDEN_2 * NUM_OUTPUT,     NUM_HIDDEN_2,   NUM_OUTPUT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数:"></a>激活函数:</h4><p>以Sigmoid函数为例(需要调用头文件 math.h):<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">double</span> <span class="title function_">Sigmoid</span><span class="params">(<span class="type">double</span> z)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+<span class="built_in">exp</span>(-z));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播:"></a>正向传播:</h4><p>按照介绍的正向传播方法进行, 读入上一层的输入, 本层的权重, 计算本一层的激活值, 再作为下一层的输入值继续这个过程.<br><strong>假设我们已经读取了数据并且存放在*feature里</strong><br>下面给出代码:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* 先初始化隐藏层与输出层的值都为0 */</span></span><br><span class="line"><span class="type">double</span> hidden1[NUM_HIDDEN_1] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="type">double</span> hidden2[NUM_HIDDEN_2] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="type">double</span> output[NUM_OUTPUT] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 计算正向传播中隐藏层和输出层的值 */</span></span><br><span class="line"><span class="comment">/* layer 需要计算的数组 num1 本层的大小 num2 输入层的大小 input 输入数据 weight 权重 bias 偏置 */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Caculate_Forward</span><span class="params">(<span class="type">double</span> *layer,<span class="type">int</span> num1, <span class="type">double</span> *input, <span class="type">int</span> num2, <span class="type">double</span> *weight, <span class="type">double</span> *bias)</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num1; i++)&#123;</span><br><span class="line">        <span class="type">double</span> sum = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; num2; j++)&#123;</span><br><span class="line">            sum = sum + input[j] * weight[i * num2 + j] + bias[i * num2 + j];</span><br><span class="line">        &#125;</span><br><span class="line">        layer[i] = Sigmoid(sum);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>之后调用这个函数进行正向传播:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">double</span> hidden1[NUM_HIDDEN_1] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="type">double</span> hidden2[NUM_HIDDEN_2] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="type">double</span> output[NUM_OUTPUT] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="comment">/* 正向传播 */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Forward</span><span class="params">()</span>&#123;</span><br><span class="line">        Caculate_Forward(hidden1, NUM_HIDDEN_1, feature, DIMENSION_SIZE, Hidden_Weight_1, Hidden_Bias_1);</span><br><span class="line">        Caculate_Forward(hidden2, NUM_HIDDEN_2, hidden1, NUM_HIDDEN_1,   Hidden_Weight_2, Hidden_Bias_2);</span><br><span class="line">        Caculate_Forward(output,  NUM_OUTPUT,   hidden2, NUM_HIDDEN_2,   Output_Weight  , Output_Bias  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样就得到了最后的预测值.</p>
<p>完整代码在反向传播神经网络5.1.</p>
<h3 id="反向传播神经网络-BP神经网络"><a href="#反向传播神经网络-BP神经网络" class="headerlink" title="反向传播神经网络(BP神经网络):"></a>反向传播神经网络(BP神经网络):</h3><p>通过了上面的正向传播的过程, 我们得到了最后输出的y值, 假如我们这是一个二元分类问题(即0,1问题), 我们可以根据y的值来判断分类结果是否正确. 如果y值&gt;0.5, 我们可以认为这个分类结果是1, 反之则为0. 但是由于权重是随机创建的, 预测的结果是一片狼藉. 此时我们就会想, 该如何重新设定权重, 让误差越来越小呢? 就是使用误差的反向传播进行权重的更新.</p>
<h4 id="什么是BP神经网络"><a href="#什么是BP神经网络" class="headerlink" title="什么是BP神经网络:"></a>什么是BP神经网络:</h4><p>反向传播神经网络(Backpropagation Neural Network)是一种应用广泛, 效果良好的人工神经网络模型. 它是一种基于梯度下降的监督学习算法, 在训练数据上通过计算损失函数的梯度, 不断调整神经网络的权重和偏置, 从而使神经网络能够逐步逼近目标函数并达到较高的准确率.<br>反向传播神经网络的核心思想是误差反向传播. 当我们人类伸手去抓住一件物品的时候, 如果第一次物品太远了或者太重了, 没抓住, 我们就会修改抓的距离或者抓的位置与力度, 从而抓住物品.<br>简单来说, 反向传播就是这样一个过程. 误差通过反向逐层传播计算, 从而得出每层的误差贡献, 然后利用梯度下降算法来调整每个节点的权重. 这样一步步的反向传递, 最终能够使模型的预测错误降到最小, 从而达到最佳的预测效果.</p>
<h4 id="BP神经网络中的一些数学推导"><a href="#BP神经网络中的一些数学推导" class="headerlink" title="BP神经网络中的一些数学推导:"></a>BP神经网络中的一些数学推导:</h4><p>前置知识: <strong>复合函数求导的链式法则</strong><br>已知二元函数$z(x,y) = f(u,v)$, $u,v$又分别是$x,y$的函数,则$z$最终是$x,y$的函数: $ z(x,y) = f(u(x,y),v(x,y)) $<br>$z$的全微分形式是:</p>
<script type="math/tex; mode=display">
\tag{1}
    dz = \frac{\partial f}{\partial u}du + \frac{\partial f}{\partial v}dv</script><p>又因为$u,v$是$x,y$的函数, 可以得到$z$关于$x,y$的全微分关系:</p>
<script type="math/tex; mode=display">
\tag{2}
    dz = (\frac{\partial f}{\partial u}\frac{\partial u}{\partial x} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial x}) dx + (\frac{\partial f}{\partial u}\frac{\partial u}{\partial y} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial y}) dy</script><p>最后根据偏导数的定义就得到了:</p>
<script type="math/tex; mode=display">
\begin{align*} 
    \tag{3}
    \frac{\partial z}{\partial x} &= \frac{\partial f}{\partial u}\frac{\partial u}{\partial x} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial x} \\
    \tag{4}
    \frac{\partial z}{\partial y} &= \frac{\partial f}{\partial u}\frac{\partial u}{\partial y} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial y}
\end{align*}</script><p>介绍完了链式法则, 我们可以正式开始误差反向传播的数学推导了.</p>
<h4 id="误差的计算"><a href="#误差的计算" class="headerlink" title="误差的计算:"></a>误差的计算:</h4><p>神经网络需要一个函数来测度模型的输出值y和真实因变量值之间的差异大小, 一般这种差异被称为残差或者误差. 误差离0越近,说明模型越好.<br>常用的误差方差:</p>
<p><strong>均方误差</strong><br>均方误差是各数据偏离真实值差值的平方和 的平均数, 也就是误差平方和的平均数.</p>
<script type="math/tex; mode=display">
    MSE = \frac{1}{n}\sum_1^n(y_i - p_i)^2</script><p>其中 $y_i$ 代表真实值, $p_i$ 代表预测值.<br>这种损失函数通常用在实数值连续变量的回归问题上，并且对于残差较大的情况给予更多的权重。</p>
<p><strong>交叉熵损失函数</strong><br>交叉熵损失函数经常用于分类问题中, 此外由于交叉熵涉及到计算每个类别的概率, 所以交叉熵几乎每次都和sigmoid(或softmax)函数一起出现.<br>这里不做详细解释, 直接给出公式:</p>
<p>二分类问题:</p>
<script type="math/tex; mode=display">
    L = \frac{1}{n}\sum_i^n L_i = \frac{1}{n}\sum_i^n-[y_i\cdot log(p_i) + (1 - y_i)\cdot log(1 - p_i)]</script><p>其中 $y_i$ 代表真实值, $p_i$ 代表预测值.<br>我们选择交叉熵损失函数, 在使用反向传播算法更新神经网络的参数时, 我们需要计算输出层的误差, 即损失函数对输出层输入加权和的导数. 由于输出层仅对最后一层（隐层或输出层）的参数有影响, 因此, 输出层的误差可以由下式计算：</p>
<p>我们先计算Sigmoid函数的导数:</p>
<p>我们知道:</p>
<script type="math/tex; mode=display">
    Sigmoid(z) = \frac{1}{1+e^{-z}}</script><p>可以推导出:</p>
<script type="math/tex; mode=display">
    Sigmoid'(z) = -\frac{1}{(1+e^{-z})^2} \times -e = \frac{e}{(1+e^{-z})^2} = Sigmoid \times (1 - Sigmoid)</script><p>应用到交叉熵函数上, 就得到了(输出层元素就一个p):</p>
<script type="math/tex; mode=display">
    \frac{\partial L}{\partial z} = p - y</script><p>接下来详细推导:<br>我们已知 $p$ 是关于 $w,b,a,z$ 的函数:</p>
<script type="math/tex; mode=display">
    p = \sum^m_j(\omega_j a_j + b_j) = \sum^m_j (\omega_j Sigmoid(z_j) + b_j)</script><p>我们先对 $p$ 对 $z_j$ 求偏导:</p>
<script type="math/tex; mode=display">
    \frac{\partial p}{\partial z_j} = \omega_j Sigmoid'(z_j) = \omega_j Sigmoid(z_j)(1-Sigmoid(z_j))</script><p>稍加整理, 我们就得到了:</p>
<script type="math/tex; mode=display">
        \frac{\partial p}{\partial z} = \omega \cdot Sigmoid(z) \cdot (1-Sigmoid(z))</script><p>因为我们输出层只有一个值, 所以可以去掉前面的平均和求和:</p>
<script type="math/tex; mode=display">
    L = -[y log(p) + (1 - y) log(1 - p)]</script><p>然后我们可以得到 $L$ 求 $z$ 的偏导:</p>
<script type="math/tex; mode=display">
    \frac{\partial L}{\partial z} = \frac{\partial L}{\partial p} \frac{\partial p}{\partial z} = -(y \frac{1}{p} - (1 - y) \frac{1}{1 - p})\frac{\partial p}{\partial z}</script><p>继续化简:</p>
<script type="math/tex; mode=display">
    \frac{\partial L}{\partial z} = -\frac{y(1-p) - (1-y)(p)}{p(1-p)} \frac{\partial p}{\partial z}</script><script type="math/tex; mode=display">
    \frac{\partial L}{\partial z} = \frac{p-y}{p(1-p)} \frac{\partial p}{\partial z} = p-y</script><p>得到了一个极其简单的式子.<br>其实在多分类问题中, 我们也会得到一样的式子, 只不过此时的 $p$ 和 $y$ 表示向量.</p>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播:"></a>反向传播:</h4><p>我们计算得到的误差为$L$, 让我们来观察输出层发生了什么:</p>
<div align="center">
<img alt="人工神经网络" src="/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/反向传播.jpg">
<p style="font-size:14px;">图4-1</p></div>


<p>图中黑线部分执行的是正向传播, 对于激活层到输出层, 我们有函数关系:</p>
<script type="math/tex; mode=display">
    L = w \cdot a + b</script><p>而对于普通层到激活层, 我们有函数关系:</p>
<script type="math/tex; mode=display">
    a = Sigmoid(z)</script><p>那么, 此时我们对$L$求关于$z$的偏导数(注意Sigmoid的求导符号):</p>
<script type="math/tex; mode=display">
    \frac{\partial L}{\partial z} = \frac{\partial L}{\partial a}\frac{\partial a}{\partial z} = \frac{\partial L}{\partial a} Sigmoid'(z)</script><p>我们设</p>
<script type="math/tex; mode=display">
    \delta = \frac{\partial L}{\partial z}</script><p>根据导数的定义, 有</p>
<script type="math/tex; mode=display">
    \Delta L = \delta \Delta z</script><p>所以我们可以发现, 如果 $\delta$ 越大, $\Delta L$ 越大, 也就表示误差越大. 因此就可以说 $\delta$ 是 $\Delta L$ 的一种反映，是误差的度量。</p>
<p>继续推导, 我们可以得到以下四个式子(以下公式中的 $a,b,z,\omega,L$ 均先看作向量):</p>
<p>输出层的误差</p>
<script type="math/tex; mode=display">
\tag{1}
    \delta ^i = \frac{\partial L}{\partial z^i} =  \frac{\partial L}{\partial a ^i} Sigmoid'(z)</script><p>误差的反向传播(相邻两层误差之间的递推关系):</p>
<script type="math/tex; mode=display">
\tag{2}
    \delta ^{i-1} = \delta ^{i} (\omega ^{i})^T \cdot Sigmoid'(z)</script><p>误差与权重的关系:</p>
<script type="math/tex; mode=display">
\tag{3}
    \frac{\partial L}{\partial \omega ^i} = \delta ^i \cdot (a^{i-1})</script><p>误差与偏置的关系:</p>
<script type="math/tex; mode=display">
\tag{4}
    \frac{\partial L}{\partial b^i} = \delta ^i</script><p>接下来将详细推导后三个公式:</p>
<h5 id="误差的反向传播"><a href="#误差的反向传播" class="headerlink" title="误差的反向传播:"></a>误差的反向传播:</h5><div align="center">
<img alt="人工神经网络" src="/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/反向传播推导1.jpg">
<p style="font-size:14px;">图4-2</p></div>


<p>我们首先来看 $z^{i}_1$ 与 $z^{i-1}_j$ 的关系:</p>
<script type="math/tex; mode=display">
    z^{i}_1 = \sum_{j=1}^m \omega^i_1 a_j^{i-1} + b^i_1 = \sum_{j=1}^m \omega^i_1 Sigmoid(z_j^{i-1}) + b^i_1</script><p>所以有:</p>
<script type="math/tex; mode=display">
    \delta ^{i-1}_1 = \frac{\partial L}{\partial z^{i-1}_1} =  \frac{\partial L}{\partial z^{i}_1} \frac{\partial z_1^{i}}{\partial z^{i-1}_1} = \delta_1^i \omega^i_1 Sigmoid'(z^{i-1}_1)</script><p>稍加整理, 我们就得到了:</p>
<script type="math/tex; mode=display">
    \delta ^{i-1} = \delta ^{i} (\omega ^{i})^T \cdot Sigmoid'(z^{i-1})</script><h5 id="误差与权重的关系"><a href="#误差与权重的关系" class="headerlink" title="误差与权重的关系:"></a>误差与权重的关系:</h5><p>先取 $z^{i}_1$ 观察, 有如下式子:</p>
<script type="math/tex; mode=display">
    z^{i}_1 = \sum_{j=1}^m (\omega^i_{1\_ j} a_j^{i-1} + b^i_{1\_ j})</script><p>所以有:</p>
<script type="math/tex; mode=display">
    \frac{\partial L}{\partial \omega ^i_1} = \frac{\partial L}{\partial z^{i}_1} \frac{\partial z^{i}_1}{\partial \omega ^i_1} = \delta ^i_1 (a^{i-1})</script><p>稍加整理, 我们就得到了:</p>
<script type="math/tex; mode=display">
    \frac{\partial L}{\partial \omega ^i} = (a^{i-1})^T \times \delta ^i</script><p>注意, 这里的结果是一个矩阵, $\omega_j$ 对应第 $j$ 列.</p>
<h6 id="误差与偏置的关系"><a href="#误差与偏置的关系" class="headerlink" title="误差与偏置的关系:"></a>误差与偏置的关系:</h6><p>同 4.3.2.2:</p>
<script type="math/tex; mode=display">
    \frac{\partial L}{\partial b ^i_1} = \frac{\partial L}{\partial z^{i}_1} \frac{\partial z^{i}_1}{\partial b ^i_1} = \delta ^i_1</script><p>这里的 $\delta^i_1$ 是一个向量, 每一行的值都是 $\delta^i_1$.</p>
<p>稍加整理, 我们就得到了:</p>
<script type="math/tex; mode=display">
    \frac{\partial L}{\partial b^i} = \delta ^i</script><p>这里的 $\delta^i$ 是一个矩阵, $b_j$ 对应第 $j$ 列.</p>
<p>这样误差反向传播过程中的基本实现方法已经解释清楚了, 如有纰缪, 欢迎指出讨论.</p>
<h5 id="权重和偏置的更新"><a href="#权重和偏置的更新" class="headerlink" title="权重和偏置的更新:"></a>权重和偏置的更新:</h5><p>得到了每一层的误差, 我们就需要根据误差来修改本层的权重和偏置, 使得误差越来越小, 在神经网络中最常用的就是<strong>梯度下降法</strong>.</p>
<p>简单来说, 梯度下降的过程就是不断地沿着目标函数下降(梯度方向), 朝着更小的目标函数值靠拢的过程. 在每一次迭代中, 都会根据函数的当前位置求出该位置的梯度, 并将该梯度乘以一个较小的学习率作为步长, 再以负梯度方向为基础方向进行调整目标函数, 并更新参数, 从而逐渐达到最优解.</p>
<div align="center">
<img alt="人工神经网络" src="/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/梯度下降1.jpg">
<p style="font-size:14px;">图4-3-1</p></div>


<div align="center">
<img alt="人工神经网络" src="/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/梯度下降2.jpg">
<p style="font-size:14px;">图4-3-2</p>
<p style="font-size:14px;">图片来源于网络</p>
</div>


<p>梯度下降的基本形式就是:</p>
<script type="math/tex; mode=display">
    \theta_{n+1} = \theta_n -\alpha \nabla J(\theta)</script><p>其中, $\alpha$ 代表学习率(learning rate), $J$ 是损失函数.</p>
<p>在反向传播神经网络中, 我们对权重与偏置进行梯度下降, 使得损失函数最小. 就有如下的变式:</p>
<script type="math/tex; mode=display">
\begin{align*}
\tag{1}
    \omega^i &= \omega^i - \alpha (a^{i-1})^T \times \delta ^i  \\
\tag{2}
    b^i &= b^i - \alpha \delta^i
\end{align*}</script><h3 id="如何构建一个反向传播神经网络-以c语言为例"><a href="#如何构建一个反向传播神经网络-以c语言为例" class="headerlink" title="如何构建一个反向传播神经网络(以c语言为例):"></a>如何构建一个反向传播神经网络(以c语言为例):</h3><h4 id="正向传播神经网络的搭建"><a href="#正向传播神经网络的搭建" class="headerlink" title="正向传播神经网络的搭建:"></a>正向传播神经网络的搭建:</h4><p>反向传播神经网络需要先使用正向传播神经网络模型进行输出值的计算, 再通过误差的反向传播进行权重更新, 从而实现网络学习.<br>正向传播神经网络的搭建方法这里不再赘述,直接上代码:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_HIDDEN_1 20             <span class="comment">// 隐藏层1的神经元个数</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_HIDDEN_2 10             <span class="comment">// 隐藏层2的神经元个数</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_OUTPUT 1                <span class="comment">// 输出层的个数</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 权重 */</span></span><br><span class="line"><span class="type">double</span> Hidden_Weight_1[DIMENSION_SIZE * NUM_HIDDEN_1];</span><br><span class="line"><span class="type">double</span> Hidden_Weight_2[NUM_HIDDEN_1 * NUM_HIDDEN_2];</span><br><span class="line"><span class="type">double</span> Output_Weight[NUM_HIDDEN_2 * NUM_OUTPUT];</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 偏置 */</span></span><br><span class="line"><span class="type">double</span> Hidden_Bias_1[DIMENSION_SIZE * NUM_HIDDEN_1];</span><br><span class="line"><span class="type">double</span> Hidden_Bias_2[NUM_HIDDEN_1 * NUM_HIDDEN_2];</span><br><span class="line"><span class="type">double</span> Output_Bias[NUM_HIDDEN_2 * NUM_OUTPUT];</span><br><span class="line"></span><br><span class="line"><span class="comment">/* weights 是要更改的权重 count 是weight的数组大小 num1 上一层神经元数量 num2 当前层神经元数量 */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">xavier_init</span><span class="params">(<span class="type">double</span>* weights, <span class="type">int</span> count, <span class="type">int</span> num1, <span class="type">int</span> num2)</span> &#123;</span><br><span class="line">    <span class="type">double</span> stddev = <span class="built_in">sqrt</span>(<span class="number">2.0</span> / (num1 + num2));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</span><br><span class="line">        weights[i] = stddev * rand() / RAND_MAX;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">double</span> <span class="title function_">Sigmoid</span><span class="params">(<span class="type">double</span> z)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+<span class="built_in">exp</span>(-z));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 初始化权重和偏置,使用xavier方法 */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Init_Weight</span><span class="params">()</span>&#123;</span><br><span class="line">    xavier_init(Hidden_Weight_1, DIMENSION_SIZE * NUM_HIDDEN_1, DIMENSION_SIZE, NUM_HIDDEN_1);</span><br><span class="line">    xavier_init(Hidden_Weight_2, NUM_HIDDEN_1 * NUM_HIDDEN_2,   NUM_HIDDEN_1,   NUM_HIDDEN_2);</span><br><span class="line">    xavier_init(Output_Weight,   NUM_HIDDEN_2 * NUM_OUTPUT,     NUM_HIDDEN_2,   NUM_OUTPUT);</span><br><span class="line"></span><br><span class="line">    xavier_init(Hidden_Bias_1, DIMENSION_SIZE * NUM_HIDDEN_1, DIMENSION_SIZE, NUM_HIDDEN_1);</span><br><span class="line">    xavier_init(Hidden_Bias_2, NUM_HIDDEN_1 * NUM_HIDDEN_2,   NUM_HIDDEN_1,   NUM_HIDDEN_2);</span><br><span class="line">    xavier_init(Output_Bias,   NUM_HIDDEN_2 * NUM_OUTPUT,     NUM_HIDDEN_2,   NUM_OUTPUT);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 计算正向传播中隐藏层和输出层的值 */</span></span><br><span class="line"><span class="comment">/* layer 需要计算的数组 num1 本层的大小 num2 输入层的大小 input 输入数据 weight 权重 bias 偏置 */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Caculate_Forward</span><span class="params">(<span class="type">double</span> *layer,<span class="type">int</span> num1, <span class="type">double</span> *input, <span class="type">int</span> num2, <span class="type">double</span> *weight, <span class="type">double</span> *bias)</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num1; i++)&#123;</span><br><span class="line">        <span class="type">double</span> sum = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; num2; j++)&#123;</span><br><span class="line">            sum = sum + input[j] * weight[i * num2 + j] + bias[i * num2 + j];</span><br><span class="line">        &#125;</span><br><span class="line">        layer[i] = Sigmoid(sum);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 正向传播 */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Forward</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">double</span> hidden1[NUM_HIDDEN_1] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">double</span> hidden2[NUM_HIDDEN_2] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">double</span> output[NUM_OUTPUT] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    Caculate_Forward(hidden1, NUM_HIDDEN_1, feature, DIMENSION_SIZE,Hidden_Weight_1, Hidden_Bias_1);</span><br><span class="line">    Caculate_Forward(hidden2, NUM_HIDDEN_2, hidden1, NUM_HIDDEN_1,  Hidden_Weight_2, Hidden_Bias_2);</span><br><span class="line">    Caculate_Forward(output,  NUM_OUTPUT,   hidden2, NUM_HIDDEN_2,  Output_Weight  , Output_Bias  );</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h4 id="计算每一神经元误差对于权重与函数的导数"><a href="#计算每一神经元误差对于权重与函数的导数" class="headerlink" title="计算每一神经元误差对于权重与函数的导数:"></a>计算每一神经元误差对于权重与函数的导数:</h4><p>Sigmoid_derivative的表达式已经在4.3给出.<br>给出这部分的代码:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">double</span> <span class="title function_">Sigmoid_derivative</span><span class="params">(<span class="type">double</span> z)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Sigmoid(z) * (<span class="number">1</span> - Sigmoid(z));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 计算误差grad相当于error度量 */</span></span><br><span class="line"><span class="comment">/* layer_grad 需要计算的数组 layer  num1 本层的大小 num2 输入层的大小 input 输入数据 weight 权重*/</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Caculate_grad</span><span class="params">(<span class="type">double</span> *layer_grad, <span class="type">double</span> *layer,<span class="type">int</span> num1, <span class="type">double</span> *input, <span class="type">int</span> num2,  <span class="type">double</span> *weight)</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num1; i++)&#123;</span><br><span class="line">        <span class="type">double</span> sum = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; num2; j++)&#123;</span><br><span class="line">            <span class="comment">// weight里的值要注意</span></span><br><span class="line">            sum += input[j] * weight[j * num1 + i];</span><br><span class="line">        &#125;</span><br><span class="line">        layer_grad[i] = sum * Sigmoid_derivative(layer[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="更新每一神经元的权重和偏置"><a href="#更新每一神经元的权重和偏置" class="headerlink" title="更新每一神经元的权重和偏置:"></a>更新每一神经元的权重和偏置:</h4><p>我们需要提前定义学习率的大小:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> ALPHA 0.01              <span class="comment">// 学习率</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 更新权重和偏置 */</span></span><br><span class="line"><span class="comment">/* weight 需要更新的权重 bias 需要更新的偏置 num1 本层大小 layer_gard 本层的偏导值 beforelayer 前一层的值 num2 前一层的大小*/</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Updata_Weight</span><span class="params">(<span class="type">double</span> *weights, <span class="type">double</span> *bias, <span class="type">int</span> num1, <span class="type">double</span> *layer_grad, <span class="type">double</span> *beforelayer, <span class="type">int</span> num2)</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num1; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; num2; j++)&#123;</span><br><span class="line">            weights[i * num2 + j] = weights[i * num2 + j] - ALPHA * layer_grad[i] * beforelayer[j];</span><br><span class="line">            bias[i * num2 + j] = bias[i * num2 + j] - ALPHA * layer_grad[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播:"></a>反向传播:</h4><p>将5.1和5.2与5.3结合, 我们就得到了反向传播函数:<br><strong>有一点需要注意, 反向传播中需要使用正向传播计算得到的隐藏层值, 所以我将正向传播中定义的隐藏层放在外层先定义.</strong><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">Backward</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">double</span> hidden1[NUM_HIDDEN_1] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">double</span> hidden2[NUM_HIDDEN_2] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">double</span> output[NUM_OUTPUT] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    Forward();</span><br><span class="line">    <span class="type">double</span> hidden1_grad[NUM_HIDDEN_1] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">double</span> hidden2_grad[NUM_HIDDEN_2] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">double</span> output_grad[NUM_OUTPUT] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    Caculate_error(hidden2_grad, hidden2, NUM_HIDDEN_2, output_grad,NUM_OUTPUT,   Output_Weight  );</span><br><span class="line">    Caculate_error(hidden1_grad, hidden1, NUM_HIDDEN_1, hidden2_gradNUM_HIDDEN_2, Hidden_Weight_2);</span><br><span class="line">    <span class="comment">/* 更新权重和偏置 */</span></span><br><span class="line">    Updata_Weight(Output_Weight,   Output_Bias,   NUM_OUTPUT,   output_grad,hidden2, NUM_HIDDEN_2  );</span><br><span class="line">    Updata_Weight(Hidden_Weight_2, Hidden_Bias_2, NUM_HIDDEN_2, hidden2_gradhidden1, NUM_HIDDEN_1  );</span><br><span class="line">    Updata_Weight(Hidden_Weight_1, Hidden_Bias_1, NUM_HIDDEN_1, hidden1_gradfeature, DIMENSION_SIZE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="多次迭代"><a href="#多次迭代" class="headerlink" title="多次迭代:"></a>多次迭代:</h4><p>仅仅一次的更新权重肯定无法得到最优解, 所以我们需要多次迭代才能得到最优解:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_ITERATIONS 200  <span class="comment">// 迭代次数</span></span></span><br><span class="line"><span class="comment">/* size 为读入数据的大小 */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Interation</span><span class="params">(<span class="type">int</span> size)</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; NUM_ITERATIONS; k++)&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> n = <span class="number">0</span>; n &lt; size; n++)&#123;</span><br><span class="line">            Backward();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>到此, BP神经网络就已经被我们构建完成了.</p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语:"></a>结语:</h3><p>神经网络最基础的知识就讲解清楚了,下一篇文章准备写神经网络在多分类问题与多标签问题上的解决方法,等有时间再慢慢理吧.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Linermao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/">http://example.com/2023/05/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NeuralNetwork/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Linermao's kiosk</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/images/Avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/06/28/HexoBuilding/BuildPersonalWeb/" title="如何搭建个人博客"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">如何搭建个人博客</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/28/AboutMe/AboutMe/" title="关于我"><img class="cover" src="/img/top-img/AboutMe.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">关于我</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/Avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Linermao</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Linermao"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Linermao" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:862813266@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is Linermao's Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CNeural-Network"><span class="toc-text">神经网络Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">什么是神经网络:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">正向传播神经网络:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">什么是正向传播神经网络:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E7%9A%84"><span class="toc-text">正向传播是如何进行的:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">什么是激活函数:</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%A5c%E8%AF%AD%E8%A8%80%E4%B8%BA%E4%BE%8B"><span class="toc-text">如何搭建一个正向传播神经网络(以c语言为例):</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-text">参数设置:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">数据的初始化:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">激活函数:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">正向传播:</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">反向传播神经网络(BP神经网络):</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFBP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">什么是BP神经网络:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC"><span class="toc-text">BP神经网络中的一些数学推导:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-text">误差的计算:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">反向传播:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">误差的反向传播:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%9D%83%E9%87%8D%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-text">误差与权重的关系:</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E4%B8%8E%E5%81%8F%E7%BD%AE%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-text">误差与偏置的关系:</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%92%8C%E5%81%8F%E7%BD%AE%E7%9A%84%E6%9B%B4%E6%96%B0"><span class="toc-text">权重和偏置的更新:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%A5c%E8%AF%AD%E8%A8%80%E4%B8%BA%E4%BE%8B"><span class="toc-text">如何构建一个反向传播神经网络(以c语言为例):</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%90%AD%E5%BB%BA"><span class="toc-text">正向传播神经网络的搭建:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%AF%8F%E4%B8%80%E7%A5%9E%E7%BB%8F%E5%85%83%E8%AF%AF%E5%B7%AE%E5%AF%B9%E4%BA%8E%E6%9D%83%E9%87%8D%E4%B8%8E%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-text">计算每一神经元误差对于权重与函数的导数:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0%E6%AF%8F%E4%B8%80%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E6%9D%83%E9%87%8D%E5%92%8C%E5%81%8F%E7%BD%AE"><span class="toc-text">更新每一神经元的权重和偏置:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-1"><span class="toc-text">反向传播:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E6%AC%A1%E8%BF%AD%E4%BB%A3"><span class="toc-text">多次迭代:</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AF%AD"><span class="toc-text">结语:</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/08/10/Kaggle/Kaggle_Titanic_Improved/" title="Kaggle Titanic Improved"><img src="/img/top-img/Kaggle_Titanic_Improved.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Kaggle Titanic Improved"/></a><div class="content"><a class="title" href="/2023/08/10/Kaggle/Kaggle_Titanic_Improved/" title="Kaggle Titanic Improved">Kaggle Titanic Improved</a><time datetime="2023-08-10T08:23:10.000Z" title="发表于 2023-08-10 16:23:10">2023-08-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/09/Kaggle/Kaggle_Titanic/" title="Kaggle Titanic"><img src="/img/top-img/Kaggle_Titanic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Kaggle Titanic"/></a><div class="content"><a class="title" href="/2023/08/09/Kaggle/Kaggle_Titanic/" title="Kaggle Titanic">Kaggle Titanic</a><time datetime="2023-08-09T08:23:10.000Z" title="发表于 2023-08-09 16:23:10">2023-08-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/08/Kaggle/Kaggle_Introduce/" title="Kaggle Introduce"><img src="/img/top-img/Kaggle_Introduce.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Kaggle Introduce"/></a><div class="content"><a class="title" href="/2023/08/08/Kaggle/Kaggle_Introduce/" title="Kaggle Introduce">Kaggle Introduce</a><time datetime="2023-08-08T08:23:10.000Z" title="发表于 2023-08-08 16:23:10">2023-08-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%EF%BC%88%E4%B8%83%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%AE%AD%E7%BB%83%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="（七）深度学习之训练卷积神经网络（施工中...）">（七）深度学习之训练卷积神经网络（施工中...）</a><time datetime="2023-08-03T08:23:10.000Z" title="发表于 2023-08-03 16:23:10">2023-08-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="（六）深度学习之卷积神经网络">（六）深度学习之卷积神经网络</a><time datetime="2023-07-31T08:23:10.000Z" title="发表于 2023-07-31 16:23:10">2023-07-31</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Linermao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a><br>
<a href="https://beian.miit.gov.cn/" target="_blank">浙ICP备2023022422号-1</a>
<br>
<img src = '/img/gongan.png'>
<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33030302001279" target="_blank">浙公网安备 33030302001279号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>