<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer_PaperRead | Linermao's kiosk</title><meta name="author" content="Linermao,86281366@qq.com"><meta name="copyright" content="Linermao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer Paper ReadPrefaceThis article is written to introduce the paper - ‘Attention is all you need’. 1. Attention is all you needThe google team use “Attention is all you need” as the title of t">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer_PaperRead">
<meta property="og:url" content="http://example.com/2023/12/07/Advanced_Model/Transformer_PaperRead/index.html">
<meta property="og:site_name" content="Linermao&#39;s kiosk">
<meta property="og:description" content="Transformer Paper ReadPrefaceThis article is written to introduce the paper - ‘Attention is all you need’. 1. Attention is all you needThe google team use “Attention is all you need” as the title of t">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/top-img/Advanced_Model/Transformer_PaperRead.jpeg">
<meta property="article:published_time" content="2023-12-07T08:23:10.000Z">
<meta property="article:modified_time" content="2023-12-15T16:21:05.511Z">
<meta property="article:author" content="Linermao">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/top-img/Advanced_Model/Transformer_PaperRead.jpeg"><link rel="shortcut icon" href="/images/favicon.jpg"><link rel="canonical" href="http://example.com/2023/12/07/Advanced_Model/Transformer_PaperRead/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d0547acfa82ccac854b804414ae584ec";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":100,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer_PaperRead',
  isPost: true,
  isHome: false,
  isHighlightShrink: undefined,
  isToc: true,
  postUpdate: '2023-12-16 00:21:05'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/Avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/top-img/Advanced_Model/Transformer_PaperRead.jpeg')"><nav id="nav"><span id="blog-info"><a href="/" title="Linermao's kiosk"><span class="site-name">Linermao's kiosk</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer_PaperRead</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-07T08:23:10.000Z" title="发表于 2023-12-07 16:23:10">2023-12-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-15T16:21:05.511Z" title="更新于 2023-12-16 00:21:05">2023-12-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Advanced-Model/">Advanced_Model</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer_PaperRead"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Transformer-Paper-Read"><a href="#Transformer-Paper-Read" class="headerlink" title="Transformer Paper Read"></a>Transformer Paper Read</h1><h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><p>This article is written to introduce the paper - ‘Attention is all you need’.</p>
<h2 id="1-Attention-is-all-you-need"><a href="#1-Attention-is-all-you-need" class="headerlink" title="1. Attention is all you need"></a>1. Attention is all you need</h2><p>The google team use “Attention is all you need” as the title of transformer model, it break traditional network, not use conversation block but attention block, let me try to explain what is attention.</p>
<h2 id="2-Structure"><a href="#2-Structure" class="headerlink" title="2. Structure"></a>2. Structure</h2><div align="center">
<img height="600" src="/2023/12/07/Advanced_Model/Transformer_PaperRead/Structure.jpg">
<p style="font-size:14px;">Figure-1 Structure (source: from paper)</p></div>


<p>What we need to pay attention to are <strong>Position Encoding</strong> and <strong>“the two blocks”</strong>.</p>
<p>Firstly, let’s focus on the “small block” - <strong>Encoder Block</strong>.</p>
<h3 id="2-1-Multi-Head-Attention-Layer"><a href="#2-1-Multi-Head-Attention-Layer" class="headerlink" title="2.1 Multi-Head Attention Layer"></a>2.1 Multi-Head Attention Layer</h3><p>In this part, the most puzzling is the <strong>“Multi-Head Attention Layer”</strong>.</p>
<div align="center">
<img height="300" src="/2023/12/07/Advanced_Model/Transformer_PaperRead/MultiHead.jpg">
<p style="font-size:14px;">Figure-2 Multi Head (source: from paper)</p></div>


<p>Let’s start by looking at the single Attention Layer.</p>
<p>It’s mathematical formula is (Ignore Mask-layer):</p>
<script type="math/tex; mode=display">
\tag{2-1}
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}}V)</script><p>So the Multi-Head Attention Layer is just combine N x single Attention Layer.</p>
<p>Before we device $ Q,K,V $, we define split weight $W^Q,W^K,W^V$.</p>
<p>It’s mathematical formula is:</p>
<script type="math/tex; mode=display">
\tag{2-2}
MultiHead(Q,K,V) = Concat(head_1, head_2, ..., head_n) W^O    \\
where \ head_i = Attention(Q_i W^Q, K_i W^K, V_i W^V)</script><h3 id="2-2-Encoder-Block"><a href="#2-2-Encoder-Block" class="headerlink" title="2.2 Encoder Block"></a>2.2 Encoder Block</h3><h4 id="2-2-1-Add-amp-Norm-Layer"><a href="#2-2-1-Add-amp-Norm-Layer" class="headerlink" title="2.2.1 Add &amp; Norm Layer"></a>2.2.1 Add &amp; Norm Layer</h4><p>This layer corresponds to a residual network block, where the formula is:</p>
<script type="math/tex; mode=display">
\tag{2-3}
out(x) = LayerNorm(x + MultiHead(Q,K,V))
\\
where \ Q,K,V = x,x,x</script><p>$LayerNorm()$ is a function that normalizes data to accelerate calculations.</p>
<p>By use pytorch, we can use <strong>$torch.nn.LayerNorm()$</strong> to use it.</p>
<h4 id="2-2-2-Feed-Forward-Layer"><a href="#2-2-2-Feed-Forward-Layer" class="headerlink" title="2.2.2 Feed Forward Layer"></a>2.2.2 Feed Forward Layer</h4><p>This is a forward layer that simply uses the ReLU activation function, where the formula is:</p>
<script type="math/tex; mode=display">
\tag{2-4}
out(x) = Linear(ReLU(Linear(x)))</script><p>$Linear()$ is a function that fully connected layers through linear change.</p>
<p>By use pytorch, we can use <strong>$torch.nn.Linear()$</strong> to use it.</p>
<h3 id="2-3-Decoder-Block"><a href="#2-3-Decoder-Block" class="headerlink" title="2.3 Decoder Block"></a>2.3 Decoder Block</h3><p>The only difference between Decoder and Encoder is the Mask.</p>
<p>Why Mask is used?</p>
<blockquote>
<p>This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.</p>
</blockquote>
<p>The simple fact is that because the inputs are sequentially related, we need to preserve the sequential relationship of the inputs.</p>
<p>The code section will describes how to create a Mask.</p>
<h3 id="2-4-Positional-Encoder"><a href="#2-4-Positional-Encoder" class="headerlink" title="2.4 Positional Encoder"></a>2.4 Positional Encoder</h3><p>What is Positional Encoder?</p>
<blockquote>
<p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelas the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed.</p>
</blockquote>
<p>In languages, the order of the words and their position in a sentence really matters. The meaning of the entire sentence can change if the words are re-ordered. When implementing NLP solutions, recurrent neural networks have an inbuilt mechanism that deals with the order of sequences. The transformer model, however, does not use recurrence or convolution and treats each data point as independent of the other. Hence, positional information is added to the model explicitly to retain the information regarding the order of words in a sentence. Positional encoding is the scheme through which the knowledge of the order of objects in a sequence is maintained.</p>
<div align="center">
<img height="300" src="/2023/12/07/Advanced_Model/Transformer_PaperRead/PositionalEncoding.webp">
<p style="font-size:14px;">Figure-3 Positional Encoding (source: from machinelearningmastery.com)</p></div>


<p>The smart positional encoding scheme, where each position/index is mapped to a vector. Hence, the output of the positional encoding layer is a matrix, where each row of the matrix represents an encoded object of the sequence summed with its positional information. An example of the matrix that encodes only the positional information is shown in the figure below.</p>
<p>In paper, the team use following formula:</p>
<script type="math/tex; mode=display">
\tag{2-5}
PE_{(pos,2i)} = sin(pos/n^{2i/d_{model}})
\\
PE_{(pos,2i+1)} = cos(pos/n^{2i/d_{model}})
\\
where\ n = 10000</script><p>$pos$ is the position and $i$ is the dimension.</p>
<p>Here is an example：</p>
<div align="center">
<img height="300" src="/2023/12/07/Advanced_Model/Transformer_PaperRead/PositionalEncoding.webp">
<p style="font-size:14px;">Figure-4 Example (source: from machinelearningmastery.com)</p></div>


<p>By using location coding, we can then transform data positional information into low to high frequency information.</p>
<div align="center">
<img height="200" src="/2023/12/07/Advanced_Model/Transformer_PaperRead/Positional.jpg">
<p style="font-size:14px;">Figure-4 Example (source: from machinelearningmastery.com)</p></div>


<h2 id="3-Meaning"><a href="#3-Meaning" class="headerlink" title="3. Meaning"></a>3. Meaning</h2><p>The proposed Transformer model breaks the dominance of traditional Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN) in natural language processing.Transformer is able to capture the relationship between any two elements of an input sequence regardless of their position, through the introduction of self-attention, which is difficult for RNN and CNN to achieve. position, which is difficult to achieve with RNN and CNN.</p>
<p>Because of the emergence of Transformers, more big models are abandoning the traditional CNN and RNN structures and achieving breakthrough results on more tasks.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Linermao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/12/07/Advanced_Model/Transformer_PaperRead/">http://example.com/2023/12/07/Advanced_Model/Transformer_PaperRead/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Linermao's kiosk</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/top-img/Advanced_Model/Transformer_PaperRead.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/12/08/Advanced_Model/Transformer_Code/" title="Transformer_Code"><img class="cover" src="/img/top-img/Advanced_Model/Transformer_Code.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Transformer_Code</div></div></a></div><div class="next-post pull-right"><a href="/2023/11/26/LinuxSystem/Ubuntu%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="Ubuntu系统深度学习环境搭建"><img class="cover" src="/img/top-img/LinuxSystem/Ubuntu%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Ubuntu系统深度学习环境搭建</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/Avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Linermao</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Linermao"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Linermao" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:862813266@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is Linermao's Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer-Paper-Read"><span class="toc-text">Transformer Paper Read</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Preface"><span class="toc-text">Preface</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Attention-is-all-you-need"><span class="toc-text">1. Attention is all you need</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Structure"><span class="toc-text">2. Structure</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Multi-Head-Attention-Layer"><span class="toc-text">2.1 Multi-Head Attention Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Encoder-Block"><span class="toc-text">2.2 Encoder Block</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-Add-amp-Norm-Layer"><span class="toc-text">2.2.1 Add &amp; Norm Layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-Feed-Forward-Layer"><span class="toc-text">2.2.2 Feed Forward Layer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Decoder-Block"><span class="toc-text">2.3 Decoder Block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Positional-Encoder"><span class="toc-text">2.4 Positional Encoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Meaning"><span class="toc-text">3. Meaning</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/12/17/Tricks/Tailscale/" title="Use Tailscale to control remote device（Somethings wrong, waiting...）"><img src="/img/top-img/Tricks/Tailscale.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Use Tailscale to control remote device（Somethings wrong, waiting...）"/></a><div class="content"><a class="title" href="/2023/12/17/Tricks/Tailscale/" title="Use Tailscale to control remote device（Somethings wrong, waiting...）">Use Tailscale to control remote device（Somethings wrong, waiting...）</a><time datetime="2023-12-17T08:23:10.000Z" title="发表于 2023-12-17 16:23:10">2023-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/11/Advanced_Model/ViT_PaperRead/" title="ViT_PaperRead"><img src="/img/top-img/Advanced_Model/ViT_PaperRead.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ViT_PaperRead"/></a><div class="content"><a class="title" href="/2023/12/11/Advanced_Model/ViT_PaperRead/" title="ViT_PaperRead">ViT_PaperRead</a><time datetime="2023-12-11T08:23:10.000Z" title="发表于 2023-12-11 16:23:10">2023-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/11/Advanced_Model/CLIP_PaperRead/" title="CLIP Paper read (Waiting for perfection)"><img src="/img/top-img/Advanced_Model/CLIP_PaperRead.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CLIP Paper read (Waiting for perfection)"/></a><div class="content"><a class="title" href="/2023/12/11/Advanced_Model/CLIP_PaperRead/" title="CLIP Paper read (Waiting for perfection)">CLIP Paper read (Waiting for perfection)</a><time datetime="2023-12-10T16:01:00.000Z" title="发表于 2023-12-11 00:01:00">2023-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/08/Advanced_Model/Transformer_Code/" title="Transformer_Code"><img src="/img/top-img/Advanced_Model/Transformer_Code.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer_Code"/></a><div class="content"><a class="title" href="/2023/12/08/Advanced_Model/Transformer_Code/" title="Transformer_Code">Transformer_Code</a><time datetime="2023-12-08T08:23:10.000Z" title="发表于 2023-12-08 16:23:10">2023-12-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/07/Advanced_Model/Transformer_PaperRead/" title="Transformer_PaperRead"><img src="/img/top-img/Advanced_Model/Transformer_PaperRead.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer_PaperRead"/></a><div class="content"><a class="title" href="/2023/12/07/Advanced_Model/Transformer_PaperRead/" title="Transformer_PaperRead">Transformer_PaperRead</a><time datetime="2023-12-07T08:23:10.000Z" title="发表于 2023-12-07 16:23:10">2023-12-07</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Linermao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a><br>
<a href="https://beian.miit.gov.cn/" target="_blank">浙ICP备2023022422号-1</a>
<br>
<img src = '/img/gongan.png'>
<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33030302001279" target="_blank">浙公网安备 33030302001279号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>